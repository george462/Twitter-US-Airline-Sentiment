{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05b05ab9",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:07.517843Z",
          "iopub.status.busy": "2021-12-07T14:22:07.516685Z",
          "iopub.status.idle": "2021-12-07T14:22:09.389728Z",
          "shell.execute_reply": "2021-12-07T14:22:09.388983Z",
          "shell.execute_reply.started": "2021-12-07T13:17:16.522574Z"
        },
        "papermill": {
          "duration": 1.948899,
          "end_time": "2021-12-07T14:22:09.389908",
          "exception": false,
          "start_time": "2021-12-07T14:22:07.441009",
          "status": "completed"
        },
        "tags": [],
        "id": "05b05ab9"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import os\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef9d9b6f",
      "metadata": {
        "papermill": {
          "duration": 0.068322,
          "end_time": "2021-12-07T14:22:09.530568",
          "exception": false,
          "start_time": "2021-12-07T14:22:09.462246",
          "status": "completed"
        },
        "tags": [],
        "id": "ef9d9b6f"
      },
      "source": [
        "<h1 style=\"text-align: center;\" class=\"list-group-item list-group-item-action active\">Table of Contents</h1>\n",
        "<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href = \"#1\" role=\"tab\" aria-controls=\"settings\">1. Introduction<span class=\"badge badge-primary badge-pill\"></span></a>\n",
        "<br>\n",
        "<a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href = \"#2\" role=\"tab\" aria-controls=\"settings\">2. Load a Clean Dataset<span class=\"badge badge-primary badge-pill\"></span></a>\n",
        "<br>\n",
        "<a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#3\" role=\"tab\" aria-controls=\"settings\">3. Basic Text Pre-Processing<span class=\"badge badge-primary badge-pill\"></span></a>\n",
        "<br>\n",
        "   <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#4\" role=\"tab\" aria-controls=\"settings\">4. One-Hot Encoding<span class=\"badge badge-primary badge-pill\"></span></a>\n",
        "   <br>\n",
        "  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#5\" role=\"tab\" aria-controls=\"settings\">5. Bag of Words<span class=\"badge badge-primary badge-pill\"></span></a>\n",
        "  <br>\n",
        "  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#6\" role=\"tab\" aria-controls=\"settings\">6.N grams<span class=\"badge badge-primary badge-pill\"></span></a>\n",
        "  <br>\n",
        "  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#7\" role=\"tab\" aria-controls=\"settings\">7. TF-IDF<span class=\"badge badge-primary badge-pill\"></span></a>\n",
        "  <br>\n",
        "  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#8\" role=\"tab\" aria-controls=\"settings\">8. Word2vec Word Embeddings<span class=\"badge badge-primary badge-pill\"></span></a>  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9f93fc3",
      "metadata": {
        "papermill": {
          "duration": 0.068504,
          "end_time": "2021-12-07T14:22:09.667817",
          "exception": false,
          "start_time": "2021-12-07T14:22:09.599313",
          "status": "completed"
        },
        "tags": [],
        "id": "c9f93fc3"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Introduction</h1><a id = \"1\" ></a>\n",
        "\n",
        "\n",
        "In Natural Language Processing (NLP) the conversion of raw-text to numerical form is called <b>Text Representation</b> and believe me this step is one of the most important steps in the NLP pipeline as if we feed in poor features in ML Model, we will get poor results. In computer science, this is often called “garbage in, garbage out.”\n",
        "\n",
        "<b>I observed in NLP feeding a good text representation to an ordinary algorithm will get you much farther compared to applying a topnotch algorithm to an ordinary text representation.</b>\n",
        "\n",
        "In this notebook, I will discuss various text-representation schemes with their advantages and disadvantages so that you can choose one of the schemes which suit your task most. Our main objective is to transform a given text into numerical form so that it can be fed\n",
        "into NLP and ML algorithms.\n",
        "\n",
        "![](https://www.oreilly.com/library/view/practical-natural-language/9781492054047/assets/pnlp_0301.png)\n",
        "\n",
        "In this notebook, the focus will be on the dotted box in the figure\n",
        "\n",
        "\n",
        "here write a para on the flow of the notebook later\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c88a1cb0",
      "metadata": {
        "papermill": {
          "duration": 0.068209,
          "end_time": "2021-12-07T14:22:09.804899",
          "exception": false,
          "start_time": "2021-12-07T14:22:09.736690",
          "status": "completed"
        },
        "tags": [],
        "id": "c88a1cb0"
      },
      "source": [
        "But before moving on to the Text representation step first we have to get a cleaned dataset which then has to be preprocessed. In this notebook, I will be using only a few basic steps to preprocess the text data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "732078de",
      "metadata": {
        "papermill": {
          "duration": 0.069141,
          "end_time": "2021-12-07T14:22:09.943204",
          "exception": false,
          "start_time": "2021-12-07T14:22:09.874063",
          "status": "completed"
        },
        "tags": [],
        "id": "732078de"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Load a Clean Dataset</h1><a id = \"2\" ></a>\n",
        "\n",
        "Kaggle Datasets is one of the best sources to get a clean dataset for this notebook I will be using [Twitter US Airline Sentiment](https://www.kaggle.com/crowdflower/twitter-airline-sentiment) dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "1cca1117",
      "metadata": {
        "papermill": {
          "duration": 0.233149,
          "end_time": "2021-12-07T14:22:10.245531",
          "exception": false,
          "start_time": "2021-12-07T14:22:10.012382",
          "status": "completed"
        },
        "tags": [],
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cca1117",
        "outputId": "e73ab6cb-edf4-4fc3-8e34-62418c5a9d16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/crowdflower/twitter-airline-sentiment?dataset_version_number=4...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2.55M/2.55M [00:00<00:00, 44.9MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/crowdflower/twitter-airline-sentiment/versions/4\n"
          ]
        }
      ],
      "source": [
        "#import data\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"crowdflower/twitter-airline-sentiment\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e61d2cd",
      "metadata": {
        "papermill": {
          "duration": 0.10019,
          "end_time": "2021-12-07T14:22:10.414298",
          "exception": false,
          "start_time": "2021-12-07T14:22:10.314108",
          "status": "completed"
        },
        "tags": [],
        "id": "7e61d2cd"
      },
      "outputs": [],
      "source": [
        "#top 5 tweet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c13b864",
      "metadata": {
        "papermill": {
          "duration": 0.113262,
          "end_time": "2021-12-07T14:22:10.597621",
          "exception": false,
          "start_time": "2021-12-07T14:22:10.484359",
          "status": "completed"
        },
        "tags": [],
        "id": "2c13b864"
      },
      "outputs": [],
      "source": [
        "# info"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a42104a",
      "metadata": {
        "papermill": {
          "duration": 0.306738,
          "end_time": "2021-12-07T14:22:10.973515",
          "exception": false,
          "start_time": "2021-12-07T14:22:10.666777",
          "status": "completed"
        },
        "tags": [],
        "id": "8a42104a"
      },
      "outputs": [],
      "source": [
        "# plot for airline_sentiment count for each class"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "849e4b1e",
      "metadata": {
        "papermill": {
          "duration": 0.072146,
          "end_time": "2021-12-07T14:22:11.116201",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.044055",
          "status": "completed"
        },
        "tags": [],
        "id": "849e4b1e"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Basic Text Pre-Processing</h1><a id = \"3\" ></a>\n",
        "\n",
        "Text preprocessing steps include a few essential tasks to further clean the available text data. It includes tasks like:-\n",
        "\n",
        "**1. Stop-Word Removal** : In English words like a, an, the, as, in, on, etc. are considered as stop-words so according to our requirements we can remove them to reduce vocabulary size as these words don't have some specific meaning\n",
        "\n",
        "**2. Lower Casing** : Convert all words into the lower case because the upper or lower case may not make a difference for the problem.\n",
        "And we are reducing vocabulary size by doing so.\n",
        "\n",
        "**3. Stemming** : Stemming refers to the process of removing suffixes and reducing a word to some base form such that all different variants of that word can be represented by the same form (e.g., “walk” and “walking” are both reduced to “walk”).\n",
        "\n",
        "**4. Tokenization** : NLP software typically analyzes text by breaking it up into words (tokens) and sentences.\n",
        "\n",
        "Pre-processing of the text is not the main objective of this notebook that's why I am just covering a few basic steps in a brief\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04744cce",
      "metadata": {
        "papermill": {
          "duration": 0.080846,
          "end_time": "2021-12-07T14:22:11.268478",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.187632",
          "status": "completed"
        },
        "tags": [],
        "id": "04744cce"
      },
      "outputs": [],
      "source": [
        "# First of all let's drop the columns which we don't required\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76ae080b",
      "metadata": {
        "papermill": {
          "duration": 0.082756,
          "end_time": "2021-12-07T14:22:11.421681",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.338925",
          "status": "completed"
        },
        "tags": [],
        "id": "76ae080b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76460d5b",
      "metadata": {
        "papermill": {
          "duration": 0.079454,
          "end_time": "2021-12-07T14:22:11.571716",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.492262",
          "status": "completed"
        },
        "tags": [],
        "id": "76460d5b"
      },
      "outputs": [],
      "source": [
        "#\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bff4f92b",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:11.717874Z",
          "iopub.status.busy": "2021-12-07T14:22:11.716842Z",
          "iopub.status.idle": "2021-12-07T14:22:11.852609Z",
          "shell.execute_reply": "2021-12-07T14:22:11.853330Z",
          "shell.execute_reply.started": "2021-12-07T13:17:19.000278Z"
        },
        "papermill": {
          "duration": 0.210989,
          "end_time": "2021-12-07T14:22:11.853570",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.642581",
          "status": "completed"
        },
        "tags": [],
        "id": "bff4f92b",
        "outputId": "14679f50-96de-49b3-81f3-c12fe285500c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "# 1. Stop-Word Removal\n",
        "\n",
        "# 2. Lower Casing\n",
        "\n",
        "# 3. Stemming\n",
        "\n",
        "# 4. Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78a9e697",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:12.004808Z",
          "iopub.status.busy": "2021-12-07T14:22:12.004036Z",
          "iopub.status.idle": "2021-12-07T14:22:12.008748Z",
          "shell.execute_reply": "2021-12-07T14:22:12.009714Z",
          "shell.execute_reply.started": "2021-12-07T13:17:19.103723Z"
        },
        "papermill": {
          "duration": 0.082926,
          "end_time": "2021-12-07T14:22:12.010037",
          "exception": false,
          "start_time": "2021-12-07T14:22:11.927111",
          "status": "completed"
        },
        "tags": [],
        "id": "78a9e697",
        "outputId": "93a8ea12-24c7-4616-cf1b-e046e7421c79"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Orignal Text : @VirginAmerica I &lt;3 pretty graphics. so much better than minimal iconography. :D\n",
            "\n",
            "Preprocessed Text : ['i', 'lt', '3', 'pretty', 'graphics', 'so', 'much', 'better', 'than', 'minimal', 'iconography', 'd']\n"
          ]
        }
      ],
      "source": [
        "print(f\"Orignal Text : {data.text[11]}\")\n",
        "print()\n",
        "print(f\"Preprocessed Text : {preprocess_text(data.text[11])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "805d5248",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:12.161539Z",
          "iopub.status.busy": "2021-12-07T14:22:12.159924Z",
          "iopub.status.idle": "2021-12-07T14:22:12.371282Z",
          "shell.execute_reply": "2021-12-07T14:22:12.371809Z",
          "shell.execute_reply.started": "2021-12-07T13:17:19.112119Z"
        },
        "papermill": {
          "duration": 0.286869,
          "end_time": "2021-12-07T14:22:12.372021",
          "exception": false,
          "start_time": "2021-12-07T14:22:12.085152",
          "status": "completed"
        },
        "tags": [],
        "id": "805d5248",
        "outputId": "856aefaa-e473-41c1-ec5e-5525304f648a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>airline_sentiment</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>neutral</td>\n",
              "      <td>[what, said]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>positive</td>\n",
              "      <td>[plus, you, ve, added, commercials, to, the, e...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>neutral</td>\n",
              "      <td>[i, didn, t, today, must, mean, i, need, to, t...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>negative</td>\n",
              "      <td>[it, s, really, aggressive, to, blast, obnoxio...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>negative</td>\n",
              "      <td>[and, it, s, a, really, big, bad, thing, about...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  airline_sentiment                                               text\n",
              "0           neutral                                       [what, said]\n",
              "1          positive  [plus, you, ve, added, commercials, to, the, e...\n",
              "2           neutral  [i, didn, t, today, must, mean, i, need, to, t...\n",
              "3          negative  [it, s, really, aggressive, to, blast, obnoxio...\n",
              "4          negative  [and, it, s, a, really, big, bad, thing, about..."
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.text = data.text.map(preprocess_text)\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d801a4c",
      "metadata": {
        "papermill": {
          "duration": 0.07431,
          "end_time": "2021-12-07T14:22:12.520458",
          "exception": false,
          "start_time": "2021-12-07T14:22:12.446148",
          "status": "completed"
        },
        "tags": [],
        "id": "7d801a4c"
      },
      "source": [
        "Now we have preprocessed textual data so now we can proceed further in this notebook and discuss various text representation approaches in detail"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f9edb0b",
      "metadata": {
        "papermill": {
          "duration": 0.072323,
          "end_time": "2021-12-07T14:22:12.670810",
          "exception": false,
          "start_time": "2021-12-07T14:22:12.598487",
          "status": "completed"
        },
        "tags": [],
        "id": "5f9edb0b"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">One-Hot Encoding</h1><a id = \"4\" ></a>\n",
        "\n",
        "\n",
        "In one-hot encoding, each word w in the corpus vocabulary is given a unique integer ID (wid) that is between 1 and |V|, where V is the set of the corpus vocabulary. Each word is then represented by a V dimensional binary vector of 0s and 1s. This is done via a |V| dimension vector filled with all 0s barring the index, where index = wid. At this index, we simply put a 1. The representation for individual words is then combined to form a sentence representation.\n",
        "\n",
        "Consider an Example\n",
        "\n",
        "![](https://miro.medium.com/max/886/1*_da_YknoUuryRheNS-SYWQ.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09b46ee1",
      "metadata": {
        "papermill": {
          "duration": 0.079496,
          "end_time": "2021-12-07T14:22:12.822488",
          "exception": false,
          "start_time": "2021-12-07T14:22:12.742992",
          "status": "completed"
        },
        "tags": [],
        "id": "09b46ee1"
      },
      "outputs": [],
      "source": [
        "#this is an example vocabulary just to make concept clear\n",
        "sample_vocab = ['the', 'cat', 'sat', 'on', 'mat', 'dog', 'run', 'green', 'tree']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5c360d5",
      "metadata": {
        "papermill": {
          "duration": 4.809595,
          "end_time": "2021-12-07T14:22:17.704275",
          "exception": false,
          "start_time": "2021-12-07T14:22:12.894680",
          "status": "completed"
        },
        "tags": [],
        "id": "d5c360d5"
      },
      "outputs": [],
      "source": [
        "# vocabulary of words present in dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6920feb",
      "metadata": {
        "papermill": {
          "duration": 0.08584,
          "end_time": "2021-12-07T14:22:17.875860",
          "exception": false,
          "start_time": "2021-12-07T14:22:17.790020",
          "status": "completed"
        },
        "tags": [],
        "id": "e6920feb"
      },
      "outputs": [],
      "source": [
        "#function to return one-hot representation of passed text get_onehot_representation()\n",
        "\n",
        "#try One Hot Representation for sentence \"the cat sat on the mat\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ae0d3ef",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:18.371550Z",
          "iopub.status.busy": "2021-12-07T14:22:18.370111Z",
          "iopub.status.idle": "2021-12-07T14:22:18.375456Z",
          "shell.execute_reply": "2021-12-07T14:22:18.375914Z",
          "shell.execute_reply.started": "2021-12-07T13:17:24.240142Z"
        },
        "papermill": {
          "duration": 0.12788,
          "end_time": "2021-12-07T14:22:18.376127",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.248247",
          "status": "completed"
        },
        "tags": [],
        "id": "6ae0d3ef",
        "outputId": "3e05d1fc-aec1-45d3-a0c5-8be6954bae73"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shapes of a single sentence : (15, 14276)\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d00e7b1",
      "metadata": {
        "papermill": {
          "duration": 0.08036,
          "end_time": "2021-12-07T14:22:18.529486",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.449126",
          "status": "completed"
        },
        "tags": [],
        "id": "9d00e7b1"
      },
      "outputs": [],
      "source": [
        "#one-hot representation for dataset sentences\n",
        "\n",
        "\n",
        "#if you run this cell it will give you a memory error 😂"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea0130d6",
      "metadata": {
        "papermill": {
          "duration": 0.087987,
          "end_time": "2021-12-07T14:22:18.690306",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.602319",
          "status": "completed"
        },
        "tags": [],
        "id": "ea0130d6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6c092a14",
      "metadata": {
        "papermill": {
          "duration": 0.073998,
          "end_time": "2021-12-07T14:22:18.838243",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.764245",
          "status": "completed"
        },
        "tags": [],
        "id": "6c092a14"
      },
      "source": [
        "One-hot encoding is intuitive to understand and straightforward to implement. However, it has lots of disadvantages listed below\n",
        "\n",
        "1. The size of a one-hot vector is directly proportional to the size of the vocabulary and if we consider a real-world vocabulary size it may be in millions so we can not represent a single word with a million-dimensional vector.\n",
        "\n",
        "2. One-hot representation does not give a fixed-length representation for text, i.e., the sentence with 32 words in it and 40 words in it has variable length representation. But for most learning algorithms, we need the feature vectors to be of the same length.\n",
        "\n",
        "3. One-Hot representation gives each word the same weight whether that word is important for the task or not.\n",
        "\n",
        "4. One-Hot representation does not represent the meaning of the word in a proper numerical manner as embedding vectors do. Consider an example word read, reading should have similar real-valued vector representation but in this case, they have different representations.\n",
        "\n",
        "5. Let say we train the model on some article and get the vocabulary of size 10000 but what if we use this vocabulary on that text which contains words that are not present in learned vocabulary. This is Known as **Out Of Vocabulary (OOV)** problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f2b43c7",
      "metadata": {
        "papermill": {
          "duration": 0.07376,
          "end_time": "2021-12-07T14:22:18.987246",
          "exception": false,
          "start_time": "2021-12-07T14:22:18.913486",
          "status": "completed"
        },
        "tags": [],
        "id": "3f2b43c7"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Bag of words</h1><a id = \"5\" ></a>\n",
        "\n",
        "Bag of words (BoW) is a classical text representation technique that has been used commonly in NLP, especially in text classification problems. The key idea behind it is as follows: represent the text under consideration as a bag (collection) of words while ignoring the order and context.\n",
        "\n",
        "Similar to one-hot encoding, BoW maps words to unique integer IDs between 1 and |V|. Each document in the corpus is then converted into a vector of |V| dimensions were in the ith component of the vector, i = wid, is simply the number of times the word w occurs in the document, i.e., we simply score each word in V by their occurrence count in the document.\n",
        "\n",
        "Consider an example:\n",
        "\n",
        "let say we have a vocabulary **V consisting of words --> {the, cat, sat, in, hat, with}** then the bag of word representation of a few sentences will be given as\n",
        "\n",
        "![](https://miro.medium.com/max/1400/1*3IACMnNpwVlCl8kSTJocPA.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba511c74",
      "metadata": {
        "papermill": {
          "duration": 0.221195,
          "end_time": "2021-12-07T14:22:19.281750",
          "exception": false,
          "start_time": "2021-12-07T14:22:19.060555",
          "status": "completed"
        },
        "tags": [],
        "id": "ba511c74"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "sample_bow = CountVectorizer()\n",
        "\n",
        "# sample_corpus = [['the', 'cat', 'sat'],\n",
        "#                  ['the', 'cat', 'sat', 'in', 'the', 'hat'],\n",
        "#                  ['the', 'cat', 'with', 'the', 'hat']]\n",
        "\n",
        "sample_corpus = [\"the cat sat\", \"the cat sat in the hat\", \"the cat with the hat\"]\n",
        "\n",
        "# code\n",
        "\n",
        "\n",
        "# Bag of word Representation of sentence 'the cat cat sat in the hat'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e0e573",
      "metadata": {
        "papermill": {
          "duration": 0.073905,
          "end_time": "2021-12-07T14:22:21.253882",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.179977",
          "status": "completed"
        },
        "tags": [],
        "id": "46e0e573"
      },
      "source": [
        "**Advantages of this Bag of words(BoW) encoding** :\n",
        "\n",
        "1. Like one-hot encoding, BoW is fairly simple to understand and implement.\n",
        "\n",
        "2. With this representation, documents having the same words will have their vector representations closer to each other in Euclidean space as compared to documents with completely different words.\n",
        "\n",
        "    Consider an example Where\n",
        "\n",
        "    S1 = \"cat on the mat\" --> BoW Representation --> {0 1 1 0 1 0 1} <br>\n",
        "    S2 = \"mat on the cat\" --> BoW Representation --> {0 1 1 0 1 0 1} <br>\n",
        "    S3 = \"dog in the mat\" --> BoW Representation --> {0 1 0 1 1 1 0} <br>\n",
        "\n",
        "    The distance between S1 and S2 is 0 as compared to the distance between S1 and S3, which is 2. Thus, the vector space resulting from the BoW scheme captures the semantic similarity of documents. So if two documents have a similar vocabulary, they’ll be closer to each other in the vector space and vice versa.\n",
        "\n",
        "3. We have a fixed-length encoding for any sentence of arbitrary length.\n",
        "\n",
        "**Disadvantages of this Bag of words(BoW) encoding** :\n",
        "\n",
        "1. The size of the vector increases with the size of the vocabulary as in our case it is 14238 dimensional. Thus, sparsity continues to be a problem. One way to control it is by limiting the vocabulary to n number of the most frequent words.\n",
        "\n",
        "2. It does not capture the similarity between different words that mean the same thing. Say we have three documents: “walk”, “walked”, and “walking”. BoW vectors of all three documents will be equally apart.\n",
        "\n",
        "3. This representation does not have any way to handle **out of vocabulary (OOV)** words (i.e., new words that were not seen in the corpus that was used to build the vectorizer).\n",
        "\n",
        "4. As the name indicates, it is a “bag” of words—word order information is lost in this representation. Both S1 and S2 will have the same representation in this scheme.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be49b0ec",
      "metadata": {
        "papermill": {
          "duration": 0.074554,
          "end_time": "2021-12-07T14:22:21.402773",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.328219",
          "status": "completed"
        },
        "tags": [],
        "id": "be49b0ec"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Bag of N-Grams</h1><a id = \"6\" ></a>\n",
        "\n",
        "All the representation schemes we’ve seen so far treat words as independent units. There is no notion of phrases or word order. The bag-of-n-grams (BoN) approach tries to remedy this. It does so by breaking text into chunks of n contiguous words (or tokens). This can help us capture some context, which earlier approaches could not do. Each chunk is called an n-gram.\n",
        "\n",
        "**One can simply say Bag of words (BoW) is a special case of the Bag of n-grams having n = 1.**\n",
        "\n",
        "The corpus vocabulary, V, is then nothing but a collection of all unique n-grams across the text corpus. Then, each document in the corpus is represented by a vector of length |V|. This vector simply contains the frequency counts of n-grams present in the document and zero for the n-grams that are not present.\n",
        "\n",
        "Consider an Example:\n",
        "\n",
        "![](https://i.stack.imgur.com/8ARA1.png)\n",
        "\n",
        "\n",
        "The following code cell shows an example of a BoN representation considering 1–3 n-gram word features to represent the corpus that we’ve used so far.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dac2dbec",
      "metadata": {
        "papermill": {
          "duration": 0.087884,
          "end_time": "2021-12-07T14:22:21.565015",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.477131",
          "status": "completed"
        },
        "tags": [],
        "id": "dac2dbec"
      },
      "outputs": [],
      "source": [
        "# Bag of 1-gram (unigram)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Bag of 1-gram (unigram) Representation of sentence 'the cat cat sat in the hat'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72339ae4",
      "metadata": {
        "papermill": {
          "duration": 0.086755,
          "end_time": "2021-12-07T14:22:21.726496",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.639741",
          "status": "completed"
        },
        "tags": [],
        "id": "72339ae4"
      },
      "outputs": [],
      "source": [
        "# Bag of 2-gram (bigram)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# code\n",
        "\n",
        "\n",
        "\n",
        "# Bag of 2-gram (bigram) Representation of sentence 'the cat cat sat in the hat'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0d0129e",
      "metadata": {
        "papermill": {
          "duration": 0.088166,
          "end_time": "2021-12-07T14:22:21.890627",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.802461",
          "status": "completed"
        },
        "tags": [],
        "id": "c0d0129e"
      },
      "outputs": [],
      "source": [
        "# Bag of 3-gram (trigram)\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# code\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Bag of 3-gram (trigram) Representation of sentence 'the cat cat sat in the hat'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cafa10d",
      "metadata": {
        "papermill": {
          "duration": 0.075393,
          "end_time": "2021-12-07T14:22:22.042863",
          "exception": false,
          "start_time": "2021-12-07T14:22:21.967470",
          "status": "completed"
        },
        "tags": [],
        "id": "6cafa10d"
      },
      "source": [
        "**Here are the main advantages and disadvantages of BoN Representation:**\n",
        "\n",
        "1. It captures some context and word-order information in the form of n-grams.\n",
        "\n",
        "2. Thus, the resulting vector space can capture some semantic similarity. Documents having the same n-grams will have their vectors closer to each other in Euclidean space as compared to documents with completely different n-grams.\n",
        "\n",
        "3. As n increases, dimensionality (and therefore sparsity) only increases rapidly.\n",
        "\n",
        "4. It still provides no way to address the **out of vocabulary(OOV)** problem."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c2d4709",
      "metadata": {
        "papermill": {
          "duration": 0.07612,
          "end_time": "2021-12-07T14:22:22.195176",
          "exception": false,
          "start_time": "2021-12-07T14:22:22.119056",
          "status": "completed"
        },
        "tags": [],
        "id": "2c2d4709"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">TF-IDF</h1><a id = \"7\" ></a>\n",
        "\n",
        "\n",
        "In all the three approaches we’ve seen so far, all the words in the text are treated as equally important—there’s no notion of some words in the document being more important than others. TF-IDF, or term frequency-inverse document frequency, addresses this issue. It aims to quantify the importance of a given word relative to other words in the document and in the corpus.\n",
        "\n",
        "The intuition behind TF-IDF is as follows: if a word w appears many times in a sentence S1 but does not occur much in the rest of the Sentences Sn in the corpus, then the word w must be of great importance to the Sentence S1. The importance of w should increase in proportion to its frequency in S1 (how many times that word occurs in sentence S1), but at the same time, its importance should decrease in proportion to the word’s frequency in other Sentence Sn in the corpus. **Mathematically, this is captured using two quantities: TF and IDF. The two are then multiplied to arrive at the TF-IDF score.**\n",
        "\n",
        "**TF (term frequency) measures how often a term or word occurs in a given document.**\n",
        "\n",
        "Mathematical Expression of TF\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "**IDF (inverse document frequency)** measures the importance of the term across a corpus. In computing TF, all terms are given equal importance (weightage). However, it’s a well-known fact that stop words like is, are, am, etc., are not important, even though they occur frequently. To account for such cases, IDF weighs down the terms that are very common across a corpus and weighs up the rare terms. IDF of a term t is calculated as follows:\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "The TF-IDF score is a product of these two terms. Thus, TF-IDF score = TF * IDF. Let’s consider an example.\n",
        "\n",
        "Sentence A = The Car is Driven on the Road <br>\n",
        "Sentence B = The Truck is Driven on the highway <br>\n",
        "\n",
        "Computation of TF-IDF scores are shown below\n",
        "\n",
        "![](https://cdn-media-1.freecodecamp.org/images/1*q3qYevXqQOjJf6Pwdlx8Mw.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "decfeeec",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-12-07T14:22:22.349367Z",
          "iopub.status.busy": "2021-12-07T14:22:22.348641Z",
          "iopub.status.idle": "2021-12-07T14:22:22.362144Z",
          "shell.execute_reply": "2021-12-07T14:22:22.362714Z",
          "shell.execute_reply.started": "2021-12-07T13:17:25.340031Z"
        },
        "papermill": {
          "duration": 0.092602,
          "end_time": "2021-12-07T14:22:22.362886",
          "exception": false,
          "start_time": "2021-12-07T14:22:22.270284",
          "status": "completed"
        },
        "tags": [],
        "id": "decfeeec",
        "outputId": "2d787492-20ee-4742-f078-8d6c5331d04b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "IDF Values for sample corpus : [1.         1.28768207 1.69314718 1.28768207 1.         1.69314718]\n",
            "TF-IDF Representation for sentence 'the cat sat in the hat' :\n",
            "[[0.29903422 0.385061   0.50630894 0.385061   0.59806843 0.        ]]\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "\n",
        "\n",
        "# code\n",
        "\n",
        "\n",
        "\n",
        "# TF-IDF Representation for sentence 'the cat sat in the hat'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58840a00",
      "metadata": {
        "papermill": {
          "duration": 0.076534,
          "end_time": "2021-12-07T14:22:22.515592",
          "exception": false,
          "start_time": "2021-12-07T14:22:22.439058",
          "status": "completed"
        },
        "tags": [],
        "id": "58840a00"
      },
      "source": [
        "Similar to BoW, we can use the TF-IDF vectors to calculate the similarity between two texts using a similarity measure like Euclidean distance or cosine similarity. TF-IDF is a commonly used representation in application scenarios such as information\n",
        "retrieval and text classification. However, even though TF-IDF is better than the vectorization methods we saw earlier in terms of capturing similarities between words, **it still suffers from the curse of high dimensionality.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c38a382",
      "metadata": {
        "papermill": {
          "duration": 0.076053,
          "end_time": "2021-12-07T14:22:22.668690",
          "exception": false,
          "start_time": "2021-12-07T14:22:22.592637",
          "status": "completed"
        },
        "tags": [],
        "id": "6c38a382"
      },
      "source": [
        "**Here are the main advantages and disadvantages of TF-IDF Representation:**\n",
        "\n",
        "1. Its Implementation is not that easy as compared to techniques discussed above\n",
        "2. We have a fixed-length encoding for any sentence of arbitrary length.\n",
        "3. The feature vectors are high-dimensional representations. The dimensionality increases with the size of the vocabulary.\n",
        "4. It did capture a bit of the semantics of the sentence.\n",
        "5. They too cannot handle OOV words.\n",
        "\n",
        "With this, we come to the end of basic vectorization approaches. Now, let’s start looking at distributed representations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dff66412",
      "metadata": {
        "papermill": {
          "duration": 0.076367,
          "end_time": "2021-12-07T14:22:22.821093",
          "exception": false,
          "start_time": "2021-12-07T14:22:22.744726",
          "status": "completed"
        },
        "tags": [],
        "id": "dff66412"
      },
      "source": [
        "<h1  style=\"text-align: center\" class=\"list-group-item list-group-item-action active\">Word2vec Word Embeddings</h1><a id = \"8\" ></a>\n",
        "\n",
        "**Word Embeddings** : They are a real-valued vector representation of words that allows words with the same meaning to have similar representation. Thus we can say word embeddings are the projection of meanings of words in a real-valued vector\n",
        "\n",
        "Word2vec is a Word Embedding Technique published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.\n",
        "\n",
        "It is the representation of words that allows words with the same meaning to have similar representation, Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and works with very different meanings are far from one another.\n",
        "\n",
        "**Using Pre-trained word2vec word embeddings** <br>\n",
        "Training your own word embeddings is a pretty expensive process (in terms of both time and computing). Thankfully, for many scenarios, it’s not necessary to train your own embeddings Someone has done the hard work of training word embeddings on a large corpus, such as Wikipedia, news articles, or even the entire web, and has put words and their corresponding vectors on the web. These embeddings\n",
        "can be downloaded and used to get the vectors for the words you want.  \n",
        "\n",
        "Some of the most popular pre-trained embeddings are Word2vec by Google, GloVe by Stanford, and fasttext embeddings by Facebook, to name a few.\n",
        "\n",
        "Below code, cell demonstrates how to use pre-trained word2vec word embeddings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4958bcef",
      "metadata": {
        "papermill": {
          "duration": 0.076746,
          "end_time": "2021-12-07T14:23:43.126254",
          "exception": false,
          "start_time": "2021-12-07T14:23:43.049508",
          "status": "completed"
        },
        "tags": [],
        "id": "4958bcef"
      },
      "source": [
        "**Training our own embeddings**\n",
        "\n",
        "Now we’ll focus on training our own word embeddings. For this, we’ll look at two architectural variants that were proposed in the original Word2vec approach. The two variants are:\n",
        "\n",
        "1. Continuous bag of words (CBOW)\n",
        "2. SkipGram\n",
        "\n",
        "Both of these have a lot of similarities in many respects.\n",
        "\n",
        "Throughout this section, we’ll use the sentence “The quick brown fox jumps over the lazy dog” as our example text.\n",
        "\n",
        "**1. Continuous bag of words (CBOW)**\n",
        "\n",
        "In CBOW, the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears. Consider our example sentence we take the word “jumps” as the center word, then its context is formed by words in its vicinity. If we take the context size of 2, then for our example, the context is given by brown, fox, over, the. CBOW uses the context words to predict the target word—jumps—as shown in the below figure\n",
        "<br><br>\n",
        "\n",
        "![image-1.png](attachment:image.png)\n",
        "\n",
        "<br><br>\n",
        "Now next task is to create a training sample of the form (X, Y) for this task where X will be context words and Y will be Center word. We define the value of context window = 2 in this case.\n",
        "![image-3.png](attachment:image-2.png)\n",
        "\n",
        "<br><br>\n",
        "Now that we have the training data ready, let’s focus on the model. For this, we construct a shallow net (it’s shallow since it has a single hidden layer). We assume we want to learn D-dim word embeddings. Further, let V be the vocabulary of the text corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfa4b64d",
      "metadata": {
        "papermill": {
          "duration": 0.078419,
          "end_time": "2021-12-07T14:23:43.280598",
          "exception": false,
          "start_time": "2021-12-07T14:23:43.202179",
          "status": "completed"
        },
        "tags": [],
        "id": "dfa4b64d"
      },
      "source": [
        "![image-4.png](attachment:image.png)\n",
        "\n",
        "\n",
        "<br><br>\n",
        "The objective is to learn an embedding matrix E|V| x d.To begin with, we initialize the matrix randomly. Here, |V| is the size of corpus vocabulary and d is the dimension of the embedding. Let’s break down the shallow net in Figure layer by layer. In the input layer, indices of the words in context are used to fetch the corresponding rows from the embedding matrix E|V| x d. The vectors fetched are then added to get a single D-dim vector, and this is passed to the next layer. The next layer simply takes this d vector and multiplies it with another matrix E’d x |V|.. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses backpropagation to update both the matrices E and E’ accordingly. At the end of the training, E is the embedding matrix we wanted to learn.\n",
        "<br><br>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41c5ab1b",
      "metadata": {
        "papermill": {
          "duration": 0.086327,
          "end_time": "2021-12-07T14:23:43.464620",
          "exception": false,
          "start_time": "2021-12-07T14:23:43.378293",
          "status": "completed"
        },
        "tags": [],
        "id": "41c5ab1b"
      },
      "source": [
        "**2. SkipGram**\n",
        "\n",
        "SkipGram is very similar to CBOW, with some minor changes. In Skip‐ Gram, the task is to predict the context words from the center word. For our toy corpus with context size 2, using the center word “jumps,” we try to predict every word in context—“brown,” “fox,” “over,” “the”—as shown in the Figure below\n",
        "\n",
        "![image-5.png](attachment:image.png)\n",
        "\n",
        "Now we will create a training sample of the form (X, Y) for this task where X will be the center word and Y will be Context words.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "![image-6.png](attachment:image-2.png)\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "![image-9.png](attachment:image-3.png)\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "The shallow network used to train the SkipGram model, shown in the below Figure, is very similar to the network used for CBOW, with some minor changes. In the input layer, the index of the word in the target is used to fetch the corresponding row from the embedding matrix E|V| x d. The vectors fetched are then passed to the next layer. The next layer simply takes this d vector and multiplies it with another matrix E’d x |V|. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses backpropagation to update both the matrices E and E’ accordingly. At the end of the training, E is the embedding matrix we wanted to learn."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 853.142536,
      "end_time": "2021-12-07T14:36:10.246332",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-12-07T14:21:57.103796",
      "version": "2.3.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}